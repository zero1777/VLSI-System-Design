# -*- coding: utf-8 -*-
"""hw2c_107062107.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TzsDgH74KyZQUPU-pPRzEUPQbsWsJ1Xz

### Mount the parameters from the Google Drive
"""

# from google.colab import drive
# drive.mount("/content/drive", force_remount=True)

"""### Load test data from the CIFAR10 DATASET"""

import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)

print(type(testloader))

"""### Store the parameters from file



"""

import pandas as pd
import json
import numpy as np

# conv1_weight
conv1_weight = pd.read_csv('conv1.weight.csv', header=None)
conv1_weight = conv1_weight.to_numpy()
conv1_weight = np.reshape(conv1_weight, (6, 3, 5, 5))

# conv2_weight
conv2_weight = pd.read_csv('conv2.weight.csv', header=None)
conv2_weight = conv2_weight.to_numpy()
conv2_weight = np.reshape(conv2_weight, (16, 6, 5, 5))

# fc1_weight
fc1_weight = pd.read_csv('fc1.weight.csv', header=None)
fc1_weight = fc1_weight.to_numpy()
fc1_weight = np.reshape(fc1_weight, (120, 400))

# fc2_weight
fc2_weight = pd.read_csv('fc2.weight.csv', header=None)
fc2_weight = fc2_weight.to_numpy()
fc2_weight = np.reshape(fc2_weight, (84, 120))

# fc3_weight
fc3_weight = pd.read_csv('fc3.weight.csv', header=None)
fc3_weight = fc3_weight.to_numpy()
fc3_weight = np.reshape(fc3_weight, (10, 84))

# fc3_bias
fc3_bias = pd.read_csv('fc3.bias.csv', header=None)
fc3_bias = fc3_bias.to_numpy()
fc3_bias = np.reshape(fc3_bias, (10))

# input
input = pd.read_csv('input.csv', header=None)
input = input.to_numpy()
input = np.reshape(input, (3, 32, 32))

# output
output = pd.read_csv('output.csv', header=None)
output = output.to_numpy()
output = np.reshape(output, (1, 10))

# scale
with open('scale.json') as f:
  scale = json.load(f)

# load different layers of scale to different variable
input_scale = scale['input_scale']
conv1_output_scale = scale['conv1_output_scale']
conv2_output_scale = scale['conv2_output_scale']
fc1_output_scale = scale['fc1_output_scale']
fc2_output_scale = scale['fc2_output_scale']
fc3_output_scale = scale['fc3_output_scale']

"""### Plot partial sum

"""

from matplotlib import pyplot as plt

def plotPartialSum(data, layer):
  counts, bins = np.histogram(data)
  if layer == 1:
    plt.title('conv1 layer')
  elif layer == 2:
    plt.title('conv2 layer')
  elif layer == 3:
    plt.title('fc1 layer')
  elif layer == 4:
    plt.title('fc2 layer')
  elif layer == 5:
    plt.title('fc3_with_bias layer')
    
  plt.hist(bins[:-1], bins, weights=counts)
  plt.show()

  print("max: ", max(data))
  print("min: ", min(data))

"""### Convolution function"""

import numba as nb
from numba import cuda

@nb.jit
def convolution(input, weight, bitWidth):
  idx = 0
  channels = input.shape[0]
  width = input.shape[1]
  height = input.shape[2]
  number = weight.shape[0]
  kernel_width = weight.shape[2]
  kernel_height = weight.shape[3]
  tmp_width = width - kernel_width + 1
  tmp_height = height - kernel_height + 1
  
  output = np.zeros((number, tmp_width, tmp_height))
  # 2D conv & pointwise
  for num in range(number):
    tmp = np.zeros((channels, tmp_width, tmp_height))
    for channel in range(channels):
      for w in range(tmp_width):
        for h in range(tmp_height):
          for kw in range(kernel_width):
            for kh in range(kernel_height):
              result = input[channel, w+kw, h+kh] * weight[num, channel, kw, kh]
              tmp[channel, w, h] += result
              if tmp[channel, w, h] < - (pow(2, bitWidth)):
                tmp[channel, w, h] = - (pow(2, bitWidth))
              elif tmp[channel, w, h] > (pow(2, bitWidth)) - 1:
                tmp[channel, w, h] = (pow(2, bitWidth)) - 1
              idx += 1

    for w in range(tmp_width):
      for h in range(tmp_height):
        for channel in range(channels):
          result = tmp[channel, w, h]
          output[num, w, h] += result
          if output[num, w, h] < - (pow(2, bitWidth)):
            output[num, w, h] = - (pow(2, bitWidth))
          elif output[num, w, h] > (pow(2, bitWidth)) - 1:
            output[num, w, h] = (pow(2, bitWidth)) - 1 
          idx += 1
  
  return output

"""### Max pooling"""

@nb.jit
def maxpool(input):
  channels = int(input.shape[0])
  width = int(input.shape[1])
  height = int(input.shape[2])
  size = 2
  output = np.zeros((channels, int(width/size), int(height/size)))

  for channel in range(channels):
    for w in range(width/size):
      for h in range(height/size):
        output[channel, w, h] = np.max(input[channel, w*size:w*size+size, h*size:h*size+size])
  
  return output

"""### Fully connected"""

@nb.jit
def fc(input, weight, bitWidth):
  height = input.shape[0]
  depth = weight.shape[0]
  output = np.zeros((depth))
  idx = 0

  for dp in range(depth):
    for h in range(height):
      result = input[h] * weight[dp, h]
      output[dp] += result
      if output[dp] < - (pow(2, bitWidth)):
        output[dp] = - (pow(2, bitWidth))
      elif output[dp] > (pow(2, bitWidth)) - 1:
        output[dp] = (pow(2, bitWidth)) - 1 
      idx += 1

  return output

@nb.jit
def fc_with_bias(input, weight, bias, bitWidth):
  height = input.shape[0]
  depth = weight.shape[0]
  output = np.zeros((depth))
  idx = 0

  for dp in range(depth):
    for h in range(height):
      result = input[h] * weight[dp, h]
      output[dp] += result
      if output[dp] < - (pow(2, bitWidth)):
        output[dp] = - (pow(2, bitWidth))
      elif output[dp] > (pow(2, bitWidth)) - 1:
        output[dp] = (pow(2, bitWidth)) - 1
      idx += 1
  
  for dp in range(depth):  
    output[dp] += bias[dp]
    if output[dp] < - (pow(2, bitWidth)):
      output[dp] = - (pow(2, bitWidth))
    elif output[dp] > (pow(2, bitWidth)) - 1:
      output[dp] = (pow(2, bitWidth)) - 1
    idx += 1

  return output

"""### Model"""

def model(x, input_scale, conv1_weight, conv1_output_scale, conv2_weight, 
    conv2_output_scale, fc1_weight, fc1_output_scale, fc2_weight, fc2_output_scale, fc3_weight, fc3_bias, fc3_output_scale, 
    conv1_bitWidth, conv2_bitWidth, fc1_bitWidth, fc2_bitWidth, fc3_bitWidth):
  
  # --- partial sum --- #
  max = 127
  min = -128
  c1 = np.empty(0)
  c2 = np.empty(0)
  f1 = np.empty(0)
  f2 = np.empty(0)
  f3 = np.empty(0)

  # --- layer 1 convoultion --- #
  # quantize the input
  x = (x * input_scale).round()
  x = np.clip(x, a_min=-128, a_max=127) 
  

  # convolution & relu
  x = convolution(x, conv1_weight, conv1_bitWidth)
  c1 = np.concatenate((c1, x.reshape(-1)))
  
  x[x < 0] = 0
  # print(np.min(c1))
  
  # max pool 2d 
  x = maxpool(x)

  # quantize layer1
  x = (x * conv1_output_scale).round()
  x = np.clip(x, a_min=-128, a_max=127) 

  # --- layer 2 convoultion --- #
  # convolution & relu
  x = convolution(x, conv2_weight, conv2_bitWidth)
  c2 = np.concatenate((c2, x.reshape(-1)))
  x[x < 0] = 0
  

  # max pool 2d 
  x = maxpool(x)

  # quantize layer2
  x = (x * conv2_output_scale).round()
  x = np.clip(x, a_min=-128, a_max=127) 

  # --- flatten the tensor --- #
  x = x.reshape(-1)

  # --- layer 3 fully connected --- #
  # fully connected & relu
  x = fc(x, fc1_weight, fc1_bitWidth)
  f1 = np.concatenate((f1, x.reshape(-1)))
  x[x < 0] = 0
  

  # quantize fc1
  x = (x * fc1_output_scale).round()
  x = np.clip(x, a_min=-128, a_max=127)

  # --- layer 4 fully connected --- #
  # fully connected & relu
  x = fc(x, fc2_weight, fc2_bitWidth)
  f2 = np.concatenate((f2, x.reshape(-1)))
  x[x < 0] = 0

  # quantize fc2
  x = (x * fc2_output_scale).round()
  x = np.clip(x, a_min=-128, a_max=127) 


  # --- layer 5 fully connected with bias --- #
  # fully connected
  x = fc_with_bias(x, fc3_weight, fc3_bias, fc3_bitWidth)
  f3 = np.concatenate((f3, x.reshape(-1)))


  # quantize fc3 and bias
  x = (x * fc3_output_scale).round()
  x = np.clip(x, a_min=-128, a_max=127)

  #print(idx)
  
  return x, c1, c2, f1, f2, f3

"""### Test function"""

def test(dataloader, conv1_bitWidth, conv2_bitWidth, fc1_bitWidth, fc2_bitWidth, fc3_bitWidth, max_samples=None):
  correct = 0
  total = 0
  n_inferences = 0
  conv1 = np.array([])
  conv2 = np.array([])
  fc1 = np.array([])
  fc2 = np.array([])
  fc3 = np.array([])

  for data in dataloader:
    images, labels = data
    images = images.numpy()
    labels = labels.numpy()
    predict = np.zeros((4))
    x1, conv1_1, conv2_1, fc1_1, fc2_1, fc3_1 = model(images[0], input_scale, conv1_weight, conv1_output_scale, conv2_weight, 
      conv2_output_scale, fc1_weight, fc1_output_scale, fc2_weight, fc2_output_scale, 
      fc3_weight, fc3_bias, fc3_output_scale, conv1_bitWidth, conv2_bitWidth, fc1_bitWidth, fc2_bitWidth, fc3_bitWidth)
    x2, conv1_2, conv2_2, fc1_2, fc2_2, fc3_2 = model(images[1], input_scale, conv1_weight, conv1_output_scale, conv2_weight, 
      conv2_output_scale, fc1_weight, fc1_output_scale, fc2_weight, fc2_output_scale, 
      fc3_weight, fc3_bias, fc3_output_scale, conv1_bitWidth, conv2_bitWidth, fc1_bitWidth, fc2_bitWidth, fc3_bitWidth)
    x3, conv1_3, conv2_3, fc1_3, fc2_3, fc3_3 = model(images[2], input_scale, conv1_weight, conv1_output_scale, conv2_weight, 
      conv2_output_scale, fc1_weight, fc1_output_scale, fc2_weight, fc2_output_scale, 
      fc3_weight, fc3_bias, fc3_output_scale, conv1_bitWidth, conv2_bitWidth, fc1_bitWidth, fc2_bitWidth, fc3_bitWidth)
    x4, conv1_4, conv2_4, fc1_4, fc2_4, fc3_4 = model(images[3], input_scale, conv1_weight, conv1_output_scale, conv2_weight, 
      conv2_output_scale, fc1_weight, fc1_output_scale, fc2_weight, fc2_output_scale, 
      fc3_weight, fc3_bias, fc3_output_scale, conv1_bitWidth, conv2_bitWidth, fc1_bitWidth, fc2_bitWidth, fc3_bitWidth)
    # for i in range(images.shape[0]):
    #   x, p = model(images[i], input_scale, conv1_weight, conv1_output_scale, conv2_weight, 
    #   conv2_output_scale, fc1_weight, fc1_output_scale, fc2_weight, fc2_output_scale, 
    #   fc3_weight, fc3_bias, fc3_output_scale, bitWidth)
    #   predict[i] = np.argmax(x)
    conv1 = np.concatenate([conv1, conv1_1, conv1_2, conv1_3, conv1_4])
    # print(np.min(conv1))
    conv2 = np.concatenate([conv2, conv2_1, conv2_2, conv2_3, conv2_4])
    fc1 = np.concatenate([fc1, fc1_1, fc1_2, fc1_3, fc1_4])
    fc2 = np.concatenate([fc2, fc2_1, fc2_2, fc2_3, fc2_4])
    fc3 = np.concatenate([fc3, fc3_1, fc3_2, fc3_3, fc3_4])

    predict[0] = np.argmax(x1)
    predict[1] = np.argmax(x2)
    predict[2] = np.argmax(x3)
    predict[3] = np.argmax(x4)

    total += labels.shape[0]
    # print(total)
    correct += np.sum(predict == labels)
    if max_samples:
      n_inferences += images.shape[0]
      if n_inferences > max_samples:
          break

  # plot the distribution
  plotPartialSum(conv1, 1)
  plotPartialSum(conv2, 2)
  plotPartialSum(fc1, 3)
  plotPartialSum(fc2, 4)
  plotPartialSum(fc3, 5)
  #print(partial.shape)

  return 100 * correct / total

"""### Implementation for checking the accuracy of the bit-width reduction"""

bit = 32
conv1_bitWidth = 32
conv2_bitWidth = 32
fc1_bitWidth = 32
fc2_bitWidth = 32
fc3_bitWidth = 32
print("bit-width:", bit, " ->  accuracy : ", test(testloader, conv1_bitWidth, conv2_bitWidth, fc1_bitWidth, fc2_bitWidth, fc3_bitWidth))

"""Implementation for trainloader"""

bit = 32
conv1_bitWidth = 32
conv2_bitWidth = 32
fc1_bitWidth = 32
fc2_bitWidth = 32
fc3_bitWidth = 32
print("bit-width:", bit, " ->  accuracy : ", test(trainloader, conv1_bitWidth, conv2_bitWidth, fc1_bitWidth, fc2_bitWidth, fc3_bitWidth))

bit = 18
conv1_bitWidth = 18
conv2_bitWidth = 16
fc1_bitWidth = 16
fc2_bitWidth = 15
fc3_bitWidth = 17
print("bit-width:", bit, " ->  accuracy : ", test(testloader, conv1_bitWidth, conv2_bitWidth, fc1_bitWidth, fc2_bitWidth, fc3_bitWidth))

bit = 18
conv1_bitWidth = 18
conv2_bitWidth = 15
fc1_bitWidth = 16
fc2_bitWidth = 15
fc3_bitWidth = 17
print("bit-width:", bit, " ->  accuracy : ", test(testloader, conv1_bitWidth, conv2_bitWidth, fc1_bitWidth, fc2_bitWidth, fc3_bitWidth))

bit = 18
conv1_bitWidth = 18
conv2_bitWidth = 16
fc1_bitWidth = 17
fc2_bitWidth = 16
fc3_bitWidth = 17
print("accuracy : ", test(testloader, conv1_bitWidth-1, conv2_bitWidth-1, fc1_bitWidth-1, fc2_bitWidth-1, fc3_bitWidth-1))
print('conv1_bitWidth: ', conv1_bitWidth)
print('conv2_bitWidth: ', conv2_bitWidth)
print('fc1_bitWidth: ', fc1_bitWidth)
print('fc2_bitWidth: ', fc2_bitWidth)
print('fc3_bitWidth: ', fc3_bitWidth)

bit = 18
conv1_bitWidth = 17
conv2_bitWidth = 16
fc1_bitWidth = 17
fc2_bitWidth = 16
fc3_bitWidth = 17
print("accuracy : ", test(testloader, conv1_bitWidth-1, conv2_bitWidth-1, fc1_bitWidth-1, fc2_bitWidth-1, fc3_bitWidth-1))
print('conv1_bitWidth: ', conv1_bitWidth)
print('conv2_bitWidth: ', conv2_bitWidth)
print('fc1_bitWidth: ', fc1_bitWidth)
print('fc2_bitWidth: ', fc2_bitWidth)
print('fc3_bitWidth: ', fc3_bitWidth)