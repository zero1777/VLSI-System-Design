# -*- coding: utf-8 -*-
"""hw2b_107062107.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K2go1n-atVm2uRVaVqACna_aDfiySg3v

### Mount the parameters from the Google Drive
"""

# from google.colab import drive
# drive.mount("/content/drive", force_remount=True)

"""### Load test data from the CIFAR10 DATASET"""

import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)

print(type(testloader))

"""### Store the parameters from file



"""

import pandas as pd
import json
import numpy as np

# conv1_weight
conv1_weight = pd.read_csv('conv1.weight.csv', header=None)
conv1_weight = conv1_weight.to_numpy()
conv1_weight = np.reshape(conv1_weight, (6, 3, 5, 5))

# conv2_weight
conv2_weight = pd.read_csv('conv2.weight.csv', header=None)
conv2_weight = conv2_weight.to_numpy()
conv2_weight = np.reshape(conv2_weight, (16, 6, 5, 5))

# fc1_weight
fc1_weight = pd.read_csv('fc1.weight.csv', header=None)
fc1_weight = fc1_weight.to_numpy()
fc1_weight = np.reshape(fc1_weight, (120, 400))

# fc2_weight
fc2_weight = pd.read_csv('fc2.weight.csv', header=None)
fc2_weight = fc2_weight.to_numpy()
fc2_weight = np.reshape(fc2_weight, (84, 120))

# fc3_weight
fc3_weight = pd.read_csv('fc3.weight.csv', header=None)
fc3_weight = fc3_weight.to_numpy()
fc3_weight = np.reshape(fc3_weight, (10, 84))

# fc3_bias
fc3_bias = pd.read_csv('fc3.bias.csv', header=None)
fc3_bias = fc3_bias.to_numpy()
fc3_bias = np.reshape(fc3_bias, (10))

# input
input = pd.read_csv('input.csv', header=None)
input = input.to_numpy()
input = np.reshape(input, (3, 32, 32))

# output
output = pd.read_csv('output.csv', header=None)
output = output.to_numpy()
output = np.reshape(output, (1, 10))

# scale
with open('scale.json') as f:
  scale = json.load(f)

# load different layers of scale to different variable
input_scale = scale['input_scale']
conv1_output_scale = scale['conv1_output_scale']
conv2_output_scale = scale['conv2_output_scale']
fc1_output_scale = scale['fc1_output_scale']
fc2_output_scale = scale['fc2_output_scale']
fc3_output_scale = scale['fc3_output_scale']

"""### Plot partial sum

"""

from matplotlib import pyplot as plt

def plotPartialSum(data):
  counts, bins = np.histogram(data)
  plt.hist(bins[:-1], bins, weights=counts)

  print("max: ", max(data))
  print("min: ", min(data))

"""### Convolution function"""

import numba as nb
from numba import cuda

@nb.jit
def convolution(input, weight, bitWidth):
  idx = 0
  channels = input.shape[0]
  width = input.shape[1]
  height = input.shape[2]
  number = weight.shape[0]
  kernel_width = weight.shape[2]
  kernel_height = weight.shape[3]
  tmp_width = width - kernel_width + 1
  tmp_height = height - kernel_height + 1
  partial = np.empty(0)
  
  output = np.zeros((number, tmp_width, tmp_height))
  # 2D conv & pointwise
  for num in range(number):
    tmp = np.zeros((channels, tmp_width, tmp_height))
    for channel in range(channels):
      for w in range(tmp_width):
        for h in range(tmp_height):
          for kw in range(kernel_width):
            for kh in range(kernel_height):
              result = input[channel, w+kw, h+kh] * weight[num, channel, kw, kh]
              tmp[channel, w, h] += result
              if tmp[channel, w, h] < - (pow(2, bitWidth)):
                tmp[channel, w, h] = - (pow(2, bitWidth))
              elif tmp[channel, w, h] > (pow(2, bitWidth)) - 1:
                tmp[channel, w, h] = (pow(2, bitWidth)) - 1
              idx += 1

    for w in range(tmp_width):
      for h in range(tmp_height):
        for channel in range(channels):
          result = tmp[channel, w, h]
          output[num, w, h] += result
          if output[num, w, h] < - (pow(2, bitWidth)):
            output[num, w, h] = - (pow(2, bitWidth))
          elif output[num, w, h] > (pow(2, bitWidth)) - 1:
            output[num, w, h] = (pow(2, bitWidth)) - 1 
          idx += 1
  
  return output, partial

"""### Max pooling"""

@nb.jit
def maxpool(input):
  channels = int(input.shape[0])
  width = int(input.shape[1])
  height = int(input.shape[2])
  size = 2
  output = np.zeros((channels, int(width/size), int(height/size)))

  for channel in range(channels):
    for w in range(width/size):
      for h in range(height/size):
        output[channel, w, h] = np.max(input[channel, w*size:w*size+size, h*size:h*size+size])
  
  return output

"""### Fully connected"""

@nb.jit
def fc(input, weight, bitWidth):
  height = input.shape[0]
  depth = weight.shape[0]
  output = np.zeros((depth))
  #partial = np.zeros((partial_width))
  partial = np.empty(0)
  idx = 0

  for dp in range(depth):
    for h in range(height):
      result = input[h] * weight[dp, h]
      output[dp] += result
      if output[dp] < - (pow(2, bitWidth)):
        output[dp] = - (pow(2, bitWidth))
      elif output[dp] > (pow(2, bitWidth)) - 1:
        output[dp] = (pow(2, bitWidth)) - 1 
      #partial[idx] = result
      idx += 1

  return output, partial

@nb.jit
def fc_with_bias(input, weight, bias, bitWidth):
  height = input.shape[0]
  depth = weight.shape[0]
  output = np.zeros((depth))
  #partial = np.zeros((partial_width))
  partial = np.empty(0)
  idx = 0

  for dp in range(depth):
    for h in range(height):
      result = input[h] * weight[dp, h]
      output[dp] += result
      if output[dp] < - (pow(2, bitWidth)):
        output[dp] = - (pow(2, bitWidth))
      elif output[dp] > (pow(2, bitWidth)) - 1:
        output[dp] = (pow(2, bitWidth)) - 1
      #partial[idx] = result
      idx += 1
  
  for dp in range(depth):
    
    output[dp] += bias[dp]
    if output[dp] < - (pow(2, bitWidth)):
      output[dp] = - (pow(2, bitWidth))
    elif output[dp] > (pow(2, bitWidth)) - 1:
      output[dp] = (pow(2, bitWidth)) - 1
    #partial[idx] = bias[dp]
    idx += 1

  return output, partial

"""### Model"""

def model(x, input_scale, conv1_weight, conv1_output_scale, conv2_weight, 
    conv2_output_scale, fc1_weight, fc1_output_scale, fc2_weight, fc2_output_scale, fc3_weight, fc3_bias, fc3_output_scale, 
    bitWidth):
  
  # --- partial sum --- #
  partial = np.empty(0)
  max = 127
  min = -128

  # --- layer 1 convoultion --- #
  # quantize the input
  x = (x * input_scale).round()
  x = np.clip(x, a_min=-128, a_max=127) 
  

  # convolution & relu
  x, p = convolution(x, conv1_weight, bitWidth)
  # partial = np.concatenate((partial, x.reshape(-1)))
  x[x < 0] = 0
  
  
  # max pool 2d 
  x = maxpool(x)

  # quantize layer1
  x = (x * conv1_output_scale).round()
  x = np.clip(x, a_min=-128, a_max=127) 

  # --- layer 2 convoultion --- #
  # convolution & relu
  x, p = convolution(x, conv2_weight, bitWidth)
  #print(idx)
  # partial = np.concatenate((partial, x.reshape(-1)))
  x[x < 0] = 0
  

  # max pool 2d 
  x = maxpool(x)

  # quantize layer2
  x = (x * conv2_output_scale).round()
  x = np.clip(x, a_min=-128, a_max=127) 

  # --- flatten the tensor --- #
  x = x.reshape(-1)

  # --- layer 3 fully connected --- #
  # fully connected & relu
  x, p = fc(x, fc1_weight, bitWidth)
  #print(idx)
  partial = np.concatenate((partial, x.reshape(-1)))
  x[x < 0] = 0
  

  # quantize fc1
  x = (x * fc1_output_scale).round()
  x = np.clip(x, a_min=-128, a_max=127)

  # --- layer 4 fully connected --- #
  # fully connected & relu
  x, p = fc(x, fc2_weight, bitWidth)
  #print(idx)
  # partial = np.concatenate((partial, x.reshape(-1)))
  x[x < 0] = 0

  # quantize fc2
  x = (x * fc2_output_scale).round()
  x = np.clip(x, a_min=-128, a_max=127) 


  # --- layer 5 fully connected with bias --- #
  # fully connected
  x, p = fc_with_bias(x, fc3_weight, fc3_bias, bitWidth)
  #print(idx)
  # partial = np.concatenate((partial, x.reshape(-1)))

  # quantize fc3 and bias
  x = (x * fc3_output_scale).round()
  x = np.clip(x, a_min=-128, a_max=127)

  #print(idx)
  return x, partial

"""### Test function"""

def test(dataloader, bitWidth, max_samples=None):
  correct = 0
  total = 0
  n_inferences = 0
  partial = np.empty(0)

  for data in dataloader:
    images, labels = data
    images = images.numpy()
    labels = labels.numpy()
    predict = np.zeros((4))
    x1, p1 = model(images[0], input_scale, conv1_weight, conv1_output_scale, conv2_weight, 
      conv2_output_scale, fc1_weight, fc1_output_scale, fc2_weight, fc2_output_scale, 
      fc3_weight, fc3_bias, fc3_output_scale, bitWidth)
    x2, p2 = model(images[1], input_scale, conv1_weight, conv1_output_scale, conv2_weight, 
      conv2_output_scale, fc1_weight, fc1_output_scale, fc2_weight, fc2_output_scale, 
      fc3_weight, fc3_bias, fc3_output_scale, bitWidth)
    x3, p3 = model(images[2], input_scale, conv1_weight, conv1_output_scale, conv2_weight, 
      conv2_output_scale, fc1_weight, fc1_output_scale, fc2_weight, fc2_output_scale, 
      fc3_weight, fc3_bias, fc3_output_scale, bitWidth)
    x4, p4 = model(images[3], input_scale, conv1_weight, conv1_output_scale, conv2_weight, 
      conv2_output_scale, fc1_weight, fc1_output_scale, fc2_weight, fc2_output_scale, 
      fc3_weight, fc3_bias, fc3_output_scale, bitWidth)
    # for i in range(images.shape[0]):
    #   x, p = model(images[i], input_scale, conv1_weight, conv1_output_scale, conv2_weight, 
    #   conv2_output_scale, fc1_weight, fc1_output_scale, fc2_weight, fc2_output_scale, 
    #   fc3_weight, fc3_bias, fc3_output_scale, bitWidth)
    #   predict[i] = np.argmax(x)
    # partial = np.concatenate((partial, p1, p2, p3, p4))
    predict[0] = np.argmax(x1)
    predict[1] = np.argmax(x2)
    predict[2] = np.argmax(x3)
    predict[3] = np.argmax(x4)

    total += labels.shape[0]
    # print(total)
    correct += np.sum(predict == labels)
    if max_samples:
      n_inferences += images.shape[0]
      if n_inferences > max_samples:
          break

  # plot the distribution
  # plotPartialSum(partial)
  #print(partial.shape)

  return 100 * correct / total

"""### Implementation of checking the bit-width"""

bit = 32
print("bit-width:", bit, " ->  accuracy : ", test(testloader, bit))
for bit in range(20, 13, -1):
  print("bit-width: ", bit+1, " -> accuracy : ", test(testloader, bit))